{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 20 / 125042) loss: 5.308647, and time eclipsed: 0.00 minutes\n",
      "(Iteration 40 / 125042) loss: 4.424526, and time eclipsed: 0.02 minutes\n",
      "(Iteration 60 / 125042) loss: 4.253918, and time eclipsed: 0.03 minutes\n",
      "(Iteration 80 / 125042) loss: 4.169096, and time eclipsed: 0.03 minutes\n",
      "(Iteration 100 / 125042) loss: 4.079483, and time eclipsed: 0.05 minutes\n",
      "(Iteration 120 / 125042) loss: 3.833029, and time eclipsed: 0.07 minutes\n",
      "(Iteration 140 / 125042) loss: 3.690183, and time eclipsed: 0.07 minutes\n",
      "(Iteration 160 / 125042) loss: 3.570690, and time eclipsed: 0.08 minutes\n",
      "(Iteration 180 / 125042) loss: 3.414866, and time eclipsed: 0.10 minutes\n",
      "(Iteration 200 / 125042) loss: 3.515676, and time eclipsed: 0.10 minutes\n",
      "(Iteration 220 / 125042) loss: 3.824879, and time eclipsed: 0.12 minutes\n",
      "(Iteration 240 / 125042) loss: 3.166373, and time eclipsed: 0.13 minutes\n",
      "(Iteration 260 / 125042) loss: 3.112335, and time eclipsed: 0.15 minutes\n",
      "(Iteration 280 / 125042) loss: 3.390647, and time eclipsed: 0.17 minutes\n",
      "(Iteration 300 / 125042) loss: 3.239328, and time eclipsed: 0.17 minutes\n",
      "(Iteration 320 / 125042) loss: 3.168760, and time eclipsed: 0.18 minutes\n",
      "(Iteration 340 / 125042) loss: 3.279063, and time eclipsed: 0.20 minutes\n",
      "(Iteration 360 / 125042) loss: 3.281579, and time eclipsed: 0.22 minutes\n",
      "(Iteration 380 / 125042) loss: 3.509399, and time eclipsed: 0.22 minutes\n",
      "(Iteration 400 / 125042) loss: 3.126901, and time eclipsed: 0.23 minutes\n",
      "(Iteration 420 / 125042) loss: 3.149674, and time eclipsed: 0.25 minutes\n",
      "(Iteration 440 / 125042) loss: 3.260844, and time eclipsed: 0.25 minutes\n",
      "(Iteration 460 / 125042) loss: 3.223919, and time eclipsed: 0.27 minutes\n",
      "(Iteration 480 / 125042) loss: 3.083736, and time eclipsed: 0.28 minutes\n",
      "(Iteration 500 / 125042) loss: 3.098871, and time eclipsed: 0.28 minutes\n",
      "(Iteration 520 / 125042) loss: 2.940530, and time eclipsed: 0.30 minutes\n",
      "(Iteration 540 / 125042) loss: 2.860541, and time eclipsed: 0.32 minutes\n",
      "(Iteration 560 / 125042) loss: 2.896530, and time eclipsed: 0.33 minutes\n",
      "(Iteration 580 / 125042) loss: 2.945829, and time eclipsed: 0.33 minutes\n",
      "(Iteration 600 / 125042) loss: 2.993795, and time eclipsed: 0.35 minutes\n",
      "(Iteration 620 / 125042) loss: 2.733570, and time eclipsed: 0.37 minutes\n",
      "(Iteration 640 / 125042) loss: 2.960769, and time eclipsed: 0.37 minutes\n",
      "(Iteration 660 / 125042) loss: 3.060080, and time eclipsed: 0.40 minutes\n",
      "(Iteration 680 / 125042) loss: 3.078605, and time eclipsed: 0.40 minutes\n",
      "(Iteration 700 / 125042) loss: 2.785267, and time eclipsed: 0.40 minutes\n",
      "(Iteration 720 / 125042) loss: 3.045615, and time eclipsed: 0.43 minutes\n",
      "(Iteration 740 / 125042) loss: 3.087595, and time eclipsed: 0.43 minutes\n",
      "(Iteration 760 / 125042) loss: 2.868474, and time eclipsed: 0.45 minutes\n",
      "(Iteration 780 / 125042) loss: 2.948287, and time eclipsed: 0.47 minutes\n",
      "(Iteration 800 / 125042) loss: 2.838106, and time eclipsed: 0.47 minutes\n",
      "(Iteration 820 / 125042) loss: 2.798392, and time eclipsed: 0.48 minutes\n",
      "(Iteration 840 / 125042) loss: 3.092522, and time eclipsed: 0.50 minutes\n",
      "(Iteration 860 / 125042) loss: 2.843728, and time eclipsed: 0.52 minutes\n",
      "(Iteration 880 / 125042) loss: 2.342919, and time eclipsed: 0.52 minutes\n",
      "(Iteration 900 / 125042) loss: 2.845539, and time eclipsed: 0.53 minutes\n",
      "(Iteration 920 / 125042) loss: 3.109672, and time eclipsed: 0.55 minutes\n",
      "(Iteration 940 / 125042) loss: 2.628856, and time eclipsed: 0.55 minutes\n",
      "(Iteration 960 / 125042) loss: 2.928977, and time eclipsed: 0.57 minutes\n",
      "(Iteration 980 / 125042) loss: 3.010750, and time eclipsed: 0.58 minutes\n",
      "(Iteration 1000 / 125042) loss: 2.939198, and time eclipsed: 0.58 minutes\n",
      "(Iteration 1020 / 125042) loss: 3.014660, and time eclipsed: 0.60 minutes\n",
      "(Iteration 1040 / 125042) loss: 3.109132, and time eclipsed: 0.62 minutes\n",
      "(Iteration 1060 / 125042) loss: 2.934933, and time eclipsed: 0.63 minutes\n",
      "(Iteration 1080 / 125042) loss: 3.198064, and time eclipsed: 0.63 minutes\n",
      "(Iteration 1100 / 125042) loss: 2.622273, and time eclipsed: 0.65 minutes\n",
      "(Iteration 1120 / 125042) loss: 3.052444, and time eclipsed: 0.67 minutes\n",
      "(Iteration 1140 / 125042) loss: 3.181932, and time eclipsed: 0.67 minutes\n",
      "(Iteration 1160 / 125042) loss: 2.812084, and time eclipsed: 0.68 minutes\n",
      "(Iteration 1180 / 125042) loss: 2.886429, and time eclipsed: 0.70 minutes\n",
      "(Iteration 1200 / 125042) loss: 2.973895, and time eclipsed: 0.70 minutes\n",
      "(Iteration 1220 / 125042) loss: 2.584389, and time eclipsed: 0.72 minutes\n",
      "(Iteration 1240 / 125042) loss: 3.163311, and time eclipsed: 0.73 minutes\n",
      "(Iteration 1260 / 125042) loss: 2.993565, and time eclipsed: 0.75 minutes\n",
      "(Iteration 1280 / 125042) loss: 3.019813, and time eclipsed: 0.77 minutes\n",
      "(Iteration 1300 / 125042) loss: 2.999839, and time eclipsed: 0.77 minutes\n",
      "(Iteration 1320 / 125042) loss: 3.254044, and time eclipsed: 0.78 minutes\n",
      "(Iteration 1340 / 125042) loss: 3.157892, and time eclipsed: 0.80 minutes\n",
      "(Iteration 1360 / 125042) loss: 2.976599, and time eclipsed: 0.82 minutes\n",
      "(Iteration 1380 / 125042) loss: 2.885483, and time eclipsed: 0.82 minutes\n",
      "(Iteration 1400 / 125042) loss: 2.587150, and time eclipsed: 0.83 minutes\n",
      "(Iteration 1420 / 125042) loss: 2.910255, and time eclipsed: 0.85 minutes\n",
      "(Iteration 1440 / 125042) loss: 2.701028, and time eclipsed: 0.85 minutes\n",
      "(Iteration 1460 / 125042) loss: 2.747891, and time eclipsed: 0.87 minutes\n",
      "(Iteration 1480 / 125042) loss: 2.955861, and time eclipsed: 0.88 minutes\n",
      "(Iteration 1500 / 125042) loss: 2.633045, and time eclipsed: 0.88 minutes\n",
      "(Iteration 1520 / 125042) loss: 2.918525, and time eclipsed: 0.90 minutes\n",
      "(Iteration 1540 / 125042) loss: 2.898027, and time eclipsed: 0.92 minutes\n",
      "(Iteration 1560 / 125042) loss: 2.938450, and time eclipsed: 0.93 minutes\n",
      "(Iteration 1580 / 125042) loss: 2.384235, and time eclipsed: 0.93 minutes\n",
      "(Iteration 1600 / 125042) loss: 2.776144, and time eclipsed: 0.95 minutes\n",
      "(Iteration 1620 / 125042) loss: 2.296742, and time eclipsed: 0.97 minutes\n",
      "(Iteration 1640 / 125042) loss: 2.452697, and time eclipsed: 0.97 minutes\n",
      "(Iteration 1660 / 125042) loss: 2.474259, and time eclipsed: 0.98 minutes\n",
      "(Iteration 1680 / 125042) loss: 2.943218, and time eclipsed: 1.00 minutes\n",
      "(Iteration 1700 / 125042) loss: 2.606814, and time eclipsed: 1.00 minutes\n",
      "(Iteration 1720 / 125042) loss: 2.289563, and time eclipsed: 1.03 minutes\n",
      "(Iteration 1740 / 125042) loss: 2.873288, and time eclipsed: 1.03 minutes\n",
      "(Iteration 1760 / 125042) loss: 2.668159, and time eclipsed: 1.05 minutes\n",
      "(Iteration 1780 / 125042) loss: 2.465842, and time eclipsed: 1.07 minutes\n",
      "(Iteration 1800 / 125042) loss: 2.363004, and time eclipsed: 1.07 minutes\n",
      "(Iteration 1820 / 125042) loss: 2.542771, and time eclipsed: 1.08 minutes\n",
      "(Iteration 1840 / 125042) loss: 2.449269, and time eclipsed: 1.10 minutes\n",
      "(Iteration 1860 / 125042) loss: 2.240696, and time eclipsed: 1.12 minutes\n",
      "(Iteration 1880 / 125042) loss: 2.936688, and time eclipsed: 1.12 minutes\n",
      "(Iteration 1900 / 125042) loss: 2.510875, and time eclipsed: 1.13 minutes\n",
      "(Iteration 1920 / 125042) loss: 2.333232, and time eclipsed: 1.15 minutes\n",
      "(Iteration 1940 / 125042) loss: 2.855348, and time eclipsed: 1.15 minutes\n",
      "(Iteration 1960 / 125042) loss: 2.586360, and time eclipsed: 1.17 minutes\n",
      "(Iteration 1980 / 125042) loss: 3.185695, and time eclipsed: 1.18 minutes\n",
      "(Iteration 2000 / 125042) loss: 2.691308, and time eclipsed: 1.18 minutes\n",
      "(Iteration 2020 / 125042) loss: 2.829756, and time eclipsed: 1.20 minutes\n",
      "(Iteration 2040 / 125042) loss: 2.772291, and time eclipsed: 1.22 minutes\n",
      "(Iteration 2060 / 125042) loss: 2.699142, and time eclipsed: 1.23 minutes\n",
      "(Iteration 2080 / 125042) loss: 2.760708, and time eclipsed: 1.23 minutes\n",
      "(Iteration 2100 / 125042) loss: 2.435127, and time eclipsed: 1.25 minutes\n",
      "(Iteration 2120 / 125042) loss: 2.455848, and time eclipsed: 1.27 minutes\n",
      "(Iteration 2140 / 125042) loss: 2.777513, and time eclipsed: 1.27 minutes\n",
      "(Iteration 2160 / 125042) loss: 2.707385, and time eclipsed: 1.28 minutes\n",
      "(Iteration 2180 / 125042) loss: 3.087065, and time eclipsed: 1.30 minutes\n",
      "(Iteration 2200 / 125042) loss: 2.466617, and time eclipsed: 1.30 minutes\n",
      "(Iteration 2220 / 125042) loss: 2.860013, and time eclipsed: 1.32 minutes\n",
      "(Iteration 2240 / 125042) loss: 2.248227, and time eclipsed: 1.33 minutes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 2260 / 125042) loss: 2.670335, and time eclipsed: 1.35 minutes\n",
      "(Iteration 2280 / 125042) loss: 2.686761, and time eclipsed: 1.35 minutes\n",
      "(Iteration 2300 / 125042) loss: 2.462774, and time eclipsed: 1.37 minutes\n",
      "(Iteration 2320 / 125042) loss: 2.559999, and time eclipsed: 1.38 minutes\n",
      "(Iteration 2340 / 125042) loss: 2.380521, and time eclipsed: 1.40 minutes\n",
      "(Iteration 2360 / 125042) loss: 2.258196, and time eclipsed: 1.42 minutes\n",
      "(Iteration 2380 / 125042) loss: 2.452536, and time eclipsed: 1.42 minutes\n",
      "(Iteration 2400 / 125042) loss: 2.033193, and time eclipsed: 1.43 minutes\n",
      "(Iteration 2420 / 125042) loss: 2.492369, and time eclipsed: 1.45 minutes\n",
      "(Iteration 2440 / 125042) loss: 2.525447, and time eclipsed: 1.45 minutes\n",
      "(Iteration 2460 / 125042) loss: 2.171486, and time eclipsed: 1.48 minutes\n",
      "(Iteration 2480 / 125042) loss: 2.851013, and time eclipsed: 1.48 minutes\n",
      "(Iteration 2500 / 125042) loss: 2.372532, and time eclipsed: 1.50 minutes\n",
      "(Iteration 2520 / 125042) loss: 2.481523, and time eclipsed: 1.52 minutes\n",
      "(Iteration 2540 / 125042) loss: 2.506196, and time eclipsed: 1.52 minutes\n",
      "(Iteration 2560 / 125042) loss: 2.477437, and time eclipsed: 1.53 minutes\n",
      "(Iteration 2580 / 125042) loss: 2.573367, and time eclipsed: 1.55 minutes\n",
      "(Iteration 2600 / 125042) loss: 2.392644, and time eclipsed: 1.55 minutes\n",
      "(Iteration 2620 / 125042) loss: 2.495663, and time eclipsed: 1.57 minutes\n",
      "(Iteration 2640 / 125042) loss: 2.487646, and time eclipsed: 1.58 minutes\n",
      "(Iteration 2660 / 125042) loss: 2.482186, and time eclipsed: 1.60 minutes\n",
      "(Iteration 2680 / 125042) loss: 2.180925, and time eclipsed: 1.60 minutes\n",
      "(Iteration 2700 / 125042) loss: 2.452144, and time eclipsed: 1.62 minutes\n",
      "(Iteration 2720 / 125042) loss: 2.787388, and time eclipsed: 1.63 minutes\n",
      "(Iteration 2740 / 125042) loss: 2.406532, and time eclipsed: 1.63 minutes\n",
      "(Iteration 2760 / 125042) loss: 2.559259, and time eclipsed: 1.65 minutes\n",
      "(Iteration 2780 / 125042) loss: 2.328842, and time eclipsed: 1.67 minutes\n",
      "(Iteration 2800 / 125042) loss: 2.613206, and time eclipsed: 1.67 minutes\n",
      "(Iteration 2820 / 125042) loss: 2.184606, and time eclipsed: 1.68 minutes\n",
      "(Iteration 2840 / 125042) loss: 2.127753, and time eclipsed: 1.70 minutes\n",
      "(Iteration 2860 / 125042) loss: 2.219019, and time eclipsed: 1.72 minutes\n",
      "(Iteration 2880 / 125042) loss: 2.291028, and time eclipsed: 1.73 minutes\n",
      "(Iteration 2900 / 125042) loss: 2.648394, and time eclipsed: 1.73 minutes\n",
      "(Iteration 2920 / 125042) loss: 2.269378, and time eclipsed: 1.75 minutes\n",
      "(Iteration 2940 / 125042) loss: 2.450634, and time eclipsed: 1.77 minutes\n",
      "(Iteration 2960 / 125042) loss: 1.892223, and time eclipsed: 1.78 minutes\n",
      "(Iteration 2980 / 125042) loss: 2.756406, and time eclipsed: 1.78 minutes\n",
      "(Iteration 3000 / 125042) loss: 2.671273, and time eclipsed: 1.80 minutes\n",
      "(Iteration 3020 / 125042) loss: 2.777280, and time eclipsed: 1.82 minutes\n",
      "(Iteration 3040 / 125042) loss: 2.675300, and time eclipsed: 1.82 minutes\n",
      "(Iteration 3060 / 125042) loss: 2.750348, and time eclipsed: 1.83 minutes\n",
      "(Iteration 3080 / 125042) loss: 2.500895, and time eclipsed: 1.85 minutes\n",
      "(Iteration 3100 / 125042) loss: 2.200650, and time eclipsed: 1.85 minutes\n",
      "(Iteration 3120 / 125042) loss: 2.987005, and time eclipsed: 1.87 minutes\n",
      "(Iteration 3140 / 125042) loss: 2.547997, and time eclipsed: 1.88 minutes\n",
      "(Iteration 3160 / 125042) loss: 2.613027, and time eclipsed: 1.90 minutes\n",
      "(Iteration 3180 / 125042) loss: 2.469079, and time eclipsed: 1.90 minutes\n",
      "(Iteration 3200 / 125042) loss: 2.587201, and time eclipsed: 1.92 minutes\n",
      "(Iteration 3220 / 125042) loss: 2.430957, and time eclipsed: 1.93 minutes\n",
      "(Iteration 3240 / 125042) loss: 2.383498, and time eclipsed: 1.93 minutes\n",
      "(Iteration 3260 / 125042) loss: 2.482869, and time eclipsed: 1.95 minutes\n",
      "(Iteration 3280 / 125042) loss: 2.637777, and time eclipsed: 1.97 minutes\n",
      "(Iteration 3300 / 125042) loss: 2.369481, and time eclipsed: 1.97 minutes\n",
      "(Iteration 3320 / 125042) loss: 2.190123, and time eclipsed: 1.98 minutes\n",
      "(Iteration 3340 / 125042) loss: 2.512615, and time eclipsed: 2.00 minutes\n",
      "(Iteration 3360 / 125042) loss: 2.541822, and time eclipsed: 2.02 minutes\n",
      "(Iteration 3380 / 125042) loss: 2.344511, and time eclipsed: 2.02 minutes\n",
      "(Iteration 3400 / 125042) loss: 2.600948, and time eclipsed: 2.03 minutes\n",
      "(Iteration 3420 / 125042) loss: 2.556220, and time eclipsed: 2.05 minutes\n",
      "(Iteration 3440 / 125042) loss: 2.634526, and time eclipsed: 2.05 minutes\n",
      "(Iteration 3460 / 125042) loss: 2.303277, and time eclipsed: 2.07 minutes\n",
      "(Iteration 3480 / 125042) loss: 2.499407, and time eclipsed: 2.08 minutes\n",
      "(Iteration 3500 / 125042) loss: 2.115825, and time eclipsed: 2.08 minutes\n",
      "(Iteration 3520 / 125042) loss: 2.423003, and time eclipsed: 2.12 minutes\n",
      "(Iteration 3540 / 125042) loss: 2.632796, and time eclipsed: 2.12 minutes\n",
      "(Iteration 3560 / 125042) loss: 2.466750, and time eclipsed: 2.13 minutes\n",
      "(Iteration 3580 / 125042) loss: 2.422043, and time eclipsed: 2.15 minutes\n",
      "(Iteration 3600 / 125042) loss: 2.416242, and time eclipsed: 2.15 minutes\n",
      "(Iteration 3620 / 125042) loss: 2.649045, and time eclipsed: 2.17 minutes\n",
      "(Iteration 3640 / 125042) loss: 2.682923, and time eclipsed: 2.18 minutes\n",
      "(Iteration 3660 / 125042) loss: 2.491860, and time eclipsed: 2.20 minutes\n",
      "(Iteration 3680 / 125042) loss: 2.406268, and time eclipsed: 2.20 minutes\n",
      "(Iteration 3700 / 125042) loss: 2.420157, and time eclipsed: 2.22 minutes\n",
      "(Iteration 3720 / 125042) loss: 2.429608, and time eclipsed: 2.23 minutes\n",
      "(Iteration 3740 / 125042) loss: 2.317603, and time eclipsed: 2.23 minutes\n",
      "(Iteration 3760 / 125042) loss: 2.414011, and time eclipsed: 2.25 minutes\n",
      "(Iteration 3780 / 125042) loss: 2.459595, and time eclipsed: 2.27 minutes\n",
      "(Iteration 3800 / 125042) loss: 2.378537, and time eclipsed: 2.27 minutes\n",
      "(Iteration 3820 / 125042) loss: 2.053215, and time eclipsed: 2.28 minutes\n",
      "(Iteration 3840 / 125042) loss: 2.246683, and time eclipsed: 2.30 minutes\n",
      "(Iteration 3860 / 125042) loss: 2.308551, and time eclipsed: 2.32 minutes\n",
      "(Iteration 3880 / 125042) loss: 2.216830, and time eclipsed: 2.32 minutes\n",
      "(Iteration 3900 / 125042) loss: 2.443671, and time eclipsed: 2.33 minutes\n"
     ]
    }
   ],
   "source": [
    "# In[34]:\n",
    "from datetime import datetime \n",
    "import tensorflow as tf\n",
    "from coco_utils import load_coco_data, decode_captions, sample_coco_minibatch\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.vocab_size =1004\n",
    "        self.batch_size = 32\n",
    "        self.initializer_scale =0.08\n",
    "        self.H = 512 #hidden dimension\n",
    "        self.T = 16 # caption length\n",
    "        self.feature_len = 512\n",
    "        self.W = 512 # embedding size\n",
    "        self.num_epochs_per_decay = 1\n",
    "        self.total_instances = 400135\n",
    "        self.initial_learning_rate = 2.0\n",
    "        self.input_len = 16\n",
    "        self.clip_gradients = 5.0\n",
    "        self.num_epochs = 10\n",
    "        self.num_of_layers = 1\n",
    "\n",
    "def minibatch(data, index, batch_size,total_size, split='train'):\n",
    "    #batch_size = batch_size+1\n",
    "    begin = batch_size*index%total_size\n",
    "    end = begin+ batch_size\n",
    "    if end > total_size:\n",
    "        print(begin)\n",
    "        end = end - total_size# minus sign\n",
    "        caption_end = data['%s_captions'%split][begin:]\n",
    "        caption_first = data['%s_captions'%split][:end]\n",
    "        image_idxs_end = data['%s_image_idxs'%split][:end]\n",
    "        image_idxs_first = data['%s_image_idxs'%split][begin:]\n",
    "        image_idxs = np.append(image_idxs_end, image_idxs_first, axis=0)\n",
    "        caption = np.append(caption_end, caption_first,axis=0)\n",
    "    else:\n",
    "        caption = data['%s_captions'%split][begin:end]\n",
    "        image_idxs = data['%s_image_idxs'%split][begin:end]\n",
    "    \n",
    "    image_features = data['%s_features' % split][image_idxs]\n",
    "    urls = data['%s_urls' % split][image_idxs]\n",
    "    caption_in = caption[:,:-1]\n",
    "    caption_out = caption[:,1:]\n",
    "    mask = (caption_out != 0)\n",
    "    \n",
    "    return caption_in, caption_out, mask, image_features, urls\n",
    "\n",
    "        \n",
    "class LSTM_Model:\n",
    "    def __init__(self, mode, config):\n",
    "        self.config = config\n",
    "        self.initializer = tf.random_uniform_initializer(\n",
    "            minval=-self.config.initializer_scale,\n",
    "            maxval=self.config.initializer_scale)\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name=\"global_step\")        \n",
    "        #Mode can be \"train\", \"test\", \"infer\"\n",
    "        self.mode = mode\n",
    "        #assert error if the mode is not one of the three predesigned modes.\n",
    "        assert mode in ['train','test','infer']\n",
    "    def is_training(self):\n",
    "        return self.mode == \"train\"\n",
    "  \n",
    "    def _build_embedding(self):\n",
    "        with tf.variable_scope(\"word_embedding\"):\n",
    "            self.caption_in = tf.placeholder(tf.int32,[self.config.batch_size, self.config.input_len], name=\"caption_in\")\n",
    "            self.embed_map = tf.get_variable(name=\"embed_map\", \n",
    "                                           shape=[self.config.vocab_size, self.config.W],\n",
    "                                           initializer = self.initializer)\n",
    "            word_vectors = tf.nn.embedding_lookup(self.embed_map, self.caption_in)\n",
    "        self.word_embedding = word_vectors\n",
    "        \n",
    "        with tf.variable_scope(\"image_embedding\"):\n",
    "            self.image_feature = tf.placeholder(tf.float32,[self.config.batch_size, self.config.feature_len], name=\"image_feature\")\n",
    "            feature_embedding = tf.contrib.layers.fully_connected(\n",
    "            inputs=self.image_feature,\n",
    "            num_outputs= self.config.H,\n",
    "            activation_fn=None,\n",
    "            weights_initializer= self.initializer,\n",
    "            biases_initializer=None)\n",
    "        \n",
    "        self.feature_embedding = feature_embedding\n",
    "    \n",
    "    def _build_model(self):\n",
    "        lstm_cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "            num_units = self.config.H, state_is_tuple =True)\n",
    "        lstm_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell] *self.config.num_of_layers)\n",
    "\n",
    "        # drop out is not included\n",
    "        with tf.variable_scope(\"lstm\", initializer = self.initializer) as lstm_scope:\n",
    "            self.caption_out = tf.placeholder(tf.int32,[self.config.batch_size, self.config.input_len], name=\"caption_out\")\n",
    "            self.caption_mask = tf.placeholder(tf.int32,[self.config.batch_size, self.config.input_len], name=\"caption_mask\")\n",
    "            zero_state = lstm_cell.zero_state(\n",
    "                batch_size=self.config.batch_size,dtype=tf.float32)\n",
    "            _, initial_state = lstm_cell(self.feature_embedding, zero_state)\n",
    "\n",
    "            lstm_scope.reuse_variables()\n",
    "            sequence_len = tf.reduce_sum(self.caption_mask,1)\n",
    "            lstm_out, _ = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "                                              inputs = self.word_embedding,\n",
    "                                              sequence_length = sequence_len,\n",
    "                                              initial_state = initial_state,\n",
    "                                              dtype = tf.float32,\n",
    "                                              scope = lstm_scope)\n",
    "        #to stack batches vertically\n",
    "        lstm_out = tf.reshape(lstm_out, [-1, lstm_cell.output_size])\n",
    "\n",
    "        with tf.variable_scope(\"logits\") as logits_scope:\n",
    "            #w = tf.get_variable('w', [lstm_cell.output_size, self.config.vocab_size], initializer=self.initializer)\n",
    "            #b = tf.get_variable('b', [self.config.vocab_size], initializer=tf.constant_initializer(0.0))\n",
    "            # (Nt)*H ,H*v =Nt.V, bias is zero\n",
    "            #tf.summary.histogram(\"weights\", w)\n",
    "            #logits = tf.matmul(lstm_out,w)+b\n",
    "            #variable_summaries(w)\n",
    "            logits = tf.contrib.layers.fully_connected(\n",
    "                inputs = lstm_out,\n",
    "                num_outputs = self.config.vocab_size,\n",
    "                activation_fn = None,\n",
    "                weights_initializer = self.initializer,\n",
    "                scope = logits_scope\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            targets = tf.reshape(self.caption_out,[-1])\n",
    "            mask = tf.to_float(tf.reshape(self.caption_mask,[-1]))\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets,\n",
    "                                                     logits = logits)\n",
    "            batch_loss = tf.div(tf.reduce_sum(tf.multiply(loss, mask)),\n",
    "                                tf.reduce_sum(mask),\n",
    "                                name=\"batch_loss\")\n",
    "            tf.losses.add_loss(batch_loss)\n",
    "            self.total_loss = tf.losses.get_total_loss()\n",
    "            \n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.total_loss)\n",
    "            tf.summary.histogram(\"histogramforloss\", self.total_loss)\n",
    "            # because you have several summaries, we should merge them all\n",
    "            # into one op to make it easier to manage\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "            \n",
    "\n",
    "    def build_graph(self):\n",
    "        #tf.reset_default_graph()\n",
    "        self._build_embedding()\n",
    "        self._build_model()\n",
    "        self._create_summaries()\n",
    "\n",
    "def train_model(model, config, data):\n",
    "    \n",
    "    #g = tf.Graph()\n",
    "    #with g.as_default():\n",
    "    ################define optimizer########\n",
    "    num_batches = config.total_instances/config.batch_size\n",
    "    decay_steps = int(num_batches*config.num_epochs_per_decay)\n",
    "    learning_rate = tf.constant(config.initial_learning_rate)\n",
    "\n",
    "    learning_rate_decay_fn = None\n",
    "    def _decay_fn(learning_rate, global_step):\n",
    "        return tf.train.exponential_decay(learning_rate,\n",
    "                                         global_step,\n",
    "                                         decay_steps = decay_steps,\n",
    "                                         decay_rate=0.5,\n",
    "                                         staircase=True)\n",
    "\n",
    "    learning_rate_decay_fn = _decay_fn\n",
    "    train_op = tf.contrib.layers.optimize_loss(loss=model.total_loss,\n",
    "                                              global_step = model.global_step,\n",
    "                                              learning_rate = learning_rate,\n",
    "                                              optimizer = 'SGD',\n",
    "                                              clip_gradients = config.clip_gradients,\n",
    "                                              learning_rate_decay_fn =learning_rate_decay_fn)\n",
    "\n",
    "    ##################\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    init = tf.global_variables_initializer()\n",
    "    config_ = tf.ConfigProto()\n",
    "    config_.gpu_options.allow_growth = True\n",
    "    \n",
    "    with tf.Session(config = config_) as sess:\n",
    "        sess.run(init)\n",
    "        # if checkpoint exist, restore\n",
    "        #ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "        #if ckpt and ckpt.model_checkpoint_path:\n",
    "        #    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                    \n",
    "        # 100 epoch\n",
    "        total_runs = int((config.total_instances/config.batch_size)*config.num_epochs)\n",
    "        initial_step = model.global_step.eval()\n",
    "        \n",
    "        ### initialize summary writer\n",
    "        tf.summary.scalar(\"learing_rate\", learning_rate)\n",
    "        a = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter('./graphs/singlelayer_lstm', sess.graph)\n",
    "         \n",
    "        time_now = datetime.now()\n",
    "        for t in range(total_runs):\n",
    "\n",
    "            caption_in, caption_out, mask, image_features, urls = minibatch(data,t,config.batch_size, config.total_instances)\n",
    "            \n",
    "            # feed data\n",
    "            feed_dict = {model.image_feature: image_features, model.caption_in: caption_in, \n",
    "                        model.caption_out: caption_out, model.caption_mask: mask}\n",
    "            merge_op, _, total_loss, b = sess.run([model.summary_op, train_op, model.total_loss, a],\n",
    "                                           feed_dict = feed_dict)\n",
    "\n",
    "            writer.add_summary(merge_op, global_step=t)\n",
    "            writer.add_summary(b, global_step=t)\n",
    "            \n",
    "            # print loss infor\n",
    "            if(t+1) % 20 == 0:\n",
    "                print('(Iteration %d / %d) loss: %f, and time eclipsed: %.2f minutes' % (\n",
    "                    t + 1, total_runs, float(total_loss), (datetime.now() - time_now).seconds/60.0))\n",
    "            \n",
    "            #print image\n",
    "            \n",
    "\n",
    "            #save model\n",
    "            if(t+1)%50 == 0 or t == (total_runs-1):\n",
    "                if not os.path.exists('checkpoints/singlelayer_lstm'):\n",
    "                    os.makedirs('checkpoints/singlelayer_lstm')\n",
    "                saver.save(sess, 'checkpoints/singlelayer_lstm', t)\n",
    "        \n",
    "        # visualize embed matrix\n",
    "        #code to visualize the embeddings. uncomment the below to visualize embeddings\n",
    "        final_embed_matrix = sess.run(model.embed_map)\n",
    "        \n",
    "        # it has to variable. constants don't work here. you can't reuse model.embed_matrix\n",
    "        embedding_var = tf.Variable(final_embed_matrix[:1000], name='embedding')\n",
    "        sess.run(embedding_var.initializer)\n",
    "\n",
    "        config = projector.ProjectorConfig()\n",
    "        summary_writer = tf.summary.FileWriter('processed')\n",
    "\n",
    "        # add embedding to the config file\n",
    "        embedding = config.embeddings.add()\n",
    "        embedding.tensor_name = embedding_var.name\n",
    "        \n",
    "        # link this tensor to its metadata file, in this case the first 500 words of vocab\n",
    "#         metadata_path = './processed/matadata.tsv'\n",
    "#         if not os.path.exists(metadata_path):\n",
    "#             f = open(metadata_path, \"w\")\n",
    "#             f.close()\n",
    "        embedding.metadata_path = os.path.join('processed', 'metadata.tsv')\n",
    "\n",
    "        # saves a configuration file that TensorBoard will read during startup.\n",
    "        projector.visualize_embeddings(summary_writer, config)\n",
    "        saver_embed = tf.train.Saver([embedding_var])\n",
    "        saver_embed.save(sess, 'processed/model3.ckpt', 1)\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = Config()\n",
    "    data = load_coco_data()\n",
    "    model = LSTM_Model('train', config)\n",
    "    model.build_graph()\n",
    "    train_model(model, config, data)\n",
    "    validation(data, config.batch_size, model,)\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _run_validation(sess, data, batch_size, model, keep_prob):\n",
    "    \"\"\"\n",
    "    Make a single gradient update for batch data. \n",
    "    \"\"\"\n",
    "    # Make a minibatch of training data\n",
    "    minibatch = sample_coco_minibatch(data,\n",
    "                  batch_size=batch_size,\n",
    "                  split='val')\n",
    "    captions, features, urls = minibatch\n",
    "    \n",
    "    captions_in = captions[:, 0].reshape(-1, 1)\n",
    "    \n",
    "    state = None \n",
    "    final_preds = []\n",
    "    current_pred = captions_in\n",
    "    mask = np.zeros((batch_size, model_config.padded_length))\n",
    "    mask[:, 0] = 1\n",
    "    \n",
    "    # get initial state using image feature \n",
    "    feed_dict = {model['image_feature']: features, \n",
    "                 model['keep_prob']: keep_prob}\n",
    "    state = sess.run(model['initial_state'], feed_dict=feed_dict)\n",
    "    \n",
    "    # start to generate sentences\n",
    "    for t in range(model_config.padded_length):\n",
    "        feed_dict={model['input_seqs']: current_pred, \n",
    "                   model['initial_state']: state, \n",
    "                   model['input_mask']: mask, \n",
    "                   model['keep_prob']: keep_prob}\n",
    "            \n",
    "        current_pred, state = sess.run([model['preds'], model['final_state']], feed_dict=feed_dict)\n",
    "        \n",
    "        current_pred = current_pred.reshape(-1, 1)\n",
    "        \n",
    "        final_preds.append(current_pred)\n",
    "\n",
    "    return final_preds, urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_text_on_image(image, image_name, caption):\n",
    "    \n",
    "  # Write caption onto an image \n",
    "  \n",
    "    assert isinstance(image, np.ndarray), \"input image must be numpy.ndarray!\"\n",
    "  \n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(caption)\n",
    "    plt.savefig(image_name)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation(config):\n",
    "    temp_dir = \"./validation_results\"\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "    sess = tf.Session()\n",
    "    captions_pred, urls = _run_validation(sess, data, config.batch_size, model, 1.0) # the output is size (32, 16)\n",
    "    captions_pred = [unpack.reshape(-1, 1) for unpack in captions_pred]\n",
    "    captions_pred = np.concatenate(captions_pred, 1)\n",
    "                    \n",
    "    captions_deco = decode_captions(captions_pred, data['idx_to_word'])\n",
    "                    \n",
    "    for j in range(len(captions_deco)):\n",
    "        img_name = os.path.join(temp_dir, 'image_{}.jpg'.format(j))\n",
    "        img = image_from_url(urls[j])\n",
    "        write_text_on_image(img, img_name, captions_deco[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
