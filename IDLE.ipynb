{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (Unable to open file: name = 'coco_captioning/coco2014_captions.h5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fee981bb115d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-fee981bb115d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_coco_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oliver/CS638-Final-Project/coco_utils.py\u001b[0m in \u001b[0;36mload_coco_data\u001b[0;34m(max_train, pca_features)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mbase_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coco_captioning'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mcaption_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coco2014_captions.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oliver/miniconda3/envs/py3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oliver/miniconda3/envs/py3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/h5py_1490028290543/work/h5py/_objects.c:2846)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/h5py_1490028290543/work/h5py/_objects.c:2804)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open (/home/ilan/minonda/conda-bld/h5py_1490028290543/work/h5py/h5f.c:2123)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (Unable to open file: name = 'coco_captioning/coco2014_captions.h5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[34]:\n",
    "from datetime import datetime \n",
    "import tensorflow as tf\n",
    "from coco_utils import load_coco_data, decode_captions\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.vocab_size =1004\n",
    "        self.batch_size = 32\n",
    "        self.initializer_scale =0.08\n",
    "        self.H = 512 #hidden dimension\n",
    "        self.T = 16 # caption length\n",
    "        self.feature_len = 512\n",
    "        self.W = 512 # embedding size\n",
    "        self.num_epochs_per_decay = 8\n",
    "        self.total_instances = 400135\n",
    "        self.initial_learning_rate = 2.0\n",
    "        self.input_len = 16\n",
    "        self.clip_gradients = 5.0\n",
    "        self.num_epochs = 2\n",
    "\n",
    "def minibatch(data, batch_size,  total_instances, split='train'):\n",
    "    while(True):\n",
    "        for i in range(total_instances):\n",
    "            #size = data['%s_captions'%split].shape[0]\n",
    "            #mask = np.random.choice(size, batch_size)\n",
    "            #begin = batch_size*index\n",
    "            caption = data['%s_captions'%split][i]\n",
    "            image_idxs = data['%s_image_idxs'%split][i]\n",
    "            image_features = data['%s_features' % split][image_idxs]\n",
    "            urls = data['%s_urls' % split][image_idxs]\n",
    "            caption_in = caption[:-1]\n",
    "            caption_out = caption[1:]\n",
    "            mask = (caption_out != 0)\n",
    "            yield caption_in, caption_out, mask, image_features, urls\n",
    "\n",
    "        \n",
    "class LSTM_Model:\n",
    "    def __init__(self, mode, config):\n",
    "        self.config = config\n",
    "        self.initializer = tf.random_uniform_initializer(\n",
    "            minval=-self.config.initializer_scale,\n",
    "            maxval=self.config.initializer_scale)\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name=\"global_step\")        \n",
    "    \n",
    "  \n",
    "    def _build_embedding(self):\n",
    "        with tf.variable_scope(\"word_embedding\"):\n",
    "            self.caption_in = tf.placeholder(tf.int32,[self.config.batch_size, self.config.input_len], name=\"caption_in\")\n",
    "            self.embed_map = tf.get_variable(name=\"embed_map\", \n",
    "                                           shape=[self.config.vocab_size, self.config.W],\n",
    "                                           initializer = self.initializer)\n",
    "            word_vectors = tf.nn.embedding_lookup(self.embed_map, self.caption_in)\n",
    "        self.word_embedding = word_vectors\n",
    "        \n",
    "        with tf.variable_scope(\"image_embedding\"):\n",
    "            self.image_feature = tf.placeholder(tf.float32,[self.config.batch_size, self.config.feature_len], name=\"image_feature\")\n",
    "            feature_embedding = tf.contrib.layers.fully_connected(\n",
    "            inputs=self.image_feature,\n",
    "            num_outputs= self.config.H,\n",
    "            activation_fn=None,\n",
    "            weights_initializer= self.initializer,\n",
    "            biases_initializer=None)\n",
    "        \n",
    "        self.feature_embedding = feature_embedding\n",
    "    \n",
    "    def _build_model(self):\n",
    "        lstm_cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "            num_units = self.config.H, state_is_tuple =True)\n",
    "\n",
    "\n",
    "        # drop out is not included\n",
    "        with tf.variable_scope(\"lstm\", initializer = self.initializer) as lstm_scope:\n",
    "            self.caption_out = tf.placeholder(tf.int32,[self.config.batch_size, self.config.input_len], name=\"caption_out\")\n",
    "            self.caption_mask = tf.placeholder(tf.int32,[self.config.batch_size, self.config.input_len], name=\"caption_mask\")\n",
    "            zero_state = lstm_cell.zero_state(\n",
    "                batch_size=self.config.batch_size,dtype=tf.float32)\n",
    "            _, initial_state = lstm_cell(self.feature_embedding, zero_state)\n",
    "\n",
    "            lstm_scope.reuse_variables()\n",
    "            sequence_len = tf.reduce_sum(self.caption_mask,1)\n",
    "            lstm_out, _ = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "                                              inputs = self.word_embedding,\n",
    "                                              sequence_length = sequence_len,\n",
    "                                              initial_state = initial_state,\n",
    "                                              dtype = tf.float32,\n",
    "                                              scope = lstm_scope)\n",
    "        lstm_out = tf.reshape(lstm_out, [-1, lstm_cell.output_size])\n",
    "\n",
    "        with tf.variable_scope(\"logits\"):\n",
    "            w = tf.get_variable('w', [lstm_cell.output_size, self.config.vocab_size], initializer=self.initializer)\n",
    "            b = tf.get_variable('b', [self.config.vocab_size], initializer=tf.constant_initializer(0.0))\n",
    "            # (Nt)*H ,H*v =Nt.V, bias is zero\n",
    "            logits = tf.matmul(lstm_out,w)+b\n",
    "            #variable_summaries(w)\n",
    "            \n",
    "\n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            targets = tf.reshape(self.caption_out,[-1])\n",
    "            mask = tf.to_float(tf.reshape(self.caption_mask,[-1]))\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets,\n",
    "                                                     logits = logits)\n",
    "            batch_loss = tf.div(tf.reduce_sum(tf.multiply(loss, mask)),\n",
    "                                tf.reduce_sum(mask),\n",
    "                                name=\"batch_loss\")\n",
    "            tf.losses.add_loss(batch_loss)\n",
    "            self.total_loss = tf.losses.get_total_loss()\n",
    "            \n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.total_loss)\n",
    "            tf.summary.histogram(\"histogramforloss\", self.total_loss)\n",
    "            # because you have several summaries, we should merge them all\n",
    "            # into one op to make it easier to manage\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "            \n",
    "\n",
    "    def build_graph(self):\n",
    "        #tf.reset_default_graph()\n",
    "        self._build_embedding()\n",
    "        self._build_model()\n",
    "        self._create_summaries()\n",
    "\n",
    "def train_model(model, config, data):\n",
    "    \n",
    "    #g = tf.Graph()\n",
    "    #with g.as_default():\n",
    "    ################define optimizer########\n",
    "    num_batches = config.total_instances/config.batch_size\n",
    "    decay_steps = int(num_batches*config.num_epochs_per_decay)\n",
    "    learning_rate = tf.constant(config.initial_learning_rate)\n",
    "\n",
    "    learning_rate_decay_fn = None\n",
    "    def _decay_fn(learning_rate, global_step):\n",
    "        return tf.train.exponential_decay(learning_rate,\n",
    "                                         global_step,\n",
    "                                         decay_steps = decay_steps,\n",
    "                                         decay_rate=0.5,\n",
    "                                         staircase=True)\n",
    "\n",
    "    learning_rate_decay_fn = _decay_fn\n",
    "    train_op = tf.contrib.layers.optimize_loss(loss=model.total_loss,\n",
    "                                              global_step = model.global_step,\n",
    "                                              learning_rate = learning_rate,\n",
    "                                              optimizer = 'SGD',\n",
    "                                              clip_gradients = config.clip_gradients,\n",
    "                                              learning_rate_decay_fn =learning_rate_decay_fn)\n",
    "\n",
    "    ##################\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        # if checkpoint exist, restore\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "\n",
    "        \n",
    "        # 100 epoch\n",
    "        total_runs = int((config.total_instances/config.batch_size)*config.num_epochs)\n",
    "        initial_step = model.global_step.eval()\n",
    "        \n",
    "        ### initialize summary writer\n",
    "        tf.summary.scalar(\"learing_rate\", learning_rate)\n",
    "        a = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "        \n",
    "        #cicular iterator\n",
    "        mini = minibatch(data = data, batch_size = config.batch_size,total_instances = config.total_instances)\n",
    "        \n",
    "        time_now = datetime.now()\n",
    "        for t in range(total_runs):\n",
    "            caption_in ,caption_out ,mask ,image_features , urls = [[],[],[],[],[]]\n",
    "            for i  in range(config.batch_size):\n",
    "                caption_in_buffer, caption_out_buffer, mask_buffer, image_features_buffer, urls_buffer = next(mini)\n",
    "                caption_in.append(caption_in_buffer)\n",
    "                caption_out.append(caption_out_buffer)\n",
    "                mask.append(mask_buffer)\n",
    "                image_features.append(image_features_buffer)\n",
    "                urls.append(urls_buffer)\n",
    "            \n",
    "            \n",
    "            # feed data\n",
    "            feed_dict = {model.image_feature: image_features, model.caption_in: caption_in, \n",
    "                        model.caption_out: caption_out, model.caption_mask: mask}\n",
    "            merge_op, _, total_loss, b = sess.run([model.summary_op, train_op, model.total_loss, a],\n",
    "                                           feed_dict = feed_dict)\n",
    "\n",
    "            writer.add_summary(merge_op, global_step=t)\n",
    "            writer.add_summary(b, global_step=t)\n",
    "            \n",
    "            # print loss infor\n",
    "            if(t+1) % 20 == 0:\n",
    "                print('(Iteration %d / %d) loss: %f, and time eclipsed: %.2f minutes' % (\n",
    "                    t + 1, total_runs, float(total_loss), (datetime.now() - time_now).seconds/60.0))\n",
    "            \n",
    "            #print image\n",
    "            \n",
    "\n",
    "            #save model\n",
    "            if(t+1)%50 == 0 or t == (total_runs-1):\n",
    "                if not os.path.exists('checkpoints/lstm'):\n",
    "                    os.makedirs('checkpoints/lstm')\n",
    "                saver.save(sess, 'checkpoints/lstm', t)\n",
    "        \n",
    "        # visualize embed matrix\n",
    "        #code to visualize the embeddings. uncomment the below to visualize embeddings\n",
    "        final_embed_matrix = sess.run(model.embed_map)\n",
    "        \n",
    "        # it has to variable. constants don't work here. you can't reuse model.embed_matrix\n",
    "        embedding_var = tf.Variable(final_embed_matrix[:1000], name='embedding')\n",
    "        sess.run(embedding_var.initializer)\n",
    "\n",
    "        config = projector.ProjectorConfig()\n",
    "        summary_writer = tf.summary.FileWriter('processed')\n",
    "\n",
    "        # add embedding to the config file\n",
    "        embedding = config.embeddings.add()\n",
    "        embedding.tensor_name = embedding_var.name\n",
    "        \n",
    "        # link this tensor to its metadata file, in this case the first 500 words of vocab\n",
    "#         metadata_path = './processed/matadata.tsv'\n",
    "#         if not os.path.exists(metadata_path):\n",
    "#             f = open(metadata_path, \"w\")\n",
    "#             f.close()\n",
    "        embedding.metadata_path = os.path.join('./processed', 'metadata.tsv')\n",
    "\n",
    "        # saves a configuration file that TensorBoard will read during startup.\n",
    "        projector.visualize_embeddings(summary_writer, config)\n",
    "        saver_embed = tf.train.Saver([embedding_var])\n",
    "        saver_embed.save(sess, 'processed/model3.ckpt', 1)\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = Config()\n",
    "    data = load_coco_data()\n",
    "    model = LSTM_Model('train', config)\n",
    "    model.build_graph()\n",
    "    train_model(model, config, data)\n",
    "\n",
    "main()\n",
    "            \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "                \n",
    "        \n",
    "            \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
